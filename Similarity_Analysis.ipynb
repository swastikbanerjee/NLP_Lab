{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIdGdFAzLGcA4iYsxgeSbG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swastikbanerjee/NLP_Lab/blob/main/Similarity_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two paragraphs considered"
      ],
      "metadata": {
        "id": "1mbNaojS8nfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt1=\"The serene lake glistened under the golden rays of the setting sun, casting a mesmerizing reflection of the surrounding mountains. Trees swayed gently in the breeze, their leaves rustling softly as if whispering secrets to the evening air. A lone boat drifted lazily across the water, its oars dipping rhythmically, creating ripples that danced in the fading light.\"\n",
        "txt2=\"As the sun dipped below the horizon, the tranquil lake shimmered with a radiant glow, mirroring the majestic peaks that encircled it. Breezes swept through the forest, causing the branches to sway and sway, their whispers blending with the soothing sounds of nature. Meanwhile, a solitary boat glided gracefully across the water, its paddles slicing through the surface, leaving behind a trail of gentle waves that sparkled in the twilight.\""
      ],
      "metadata": {
        "id": "hYNRrFH-7kZJ"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text1 Preprocessing"
      ],
      "metadata": {
        "id": "EpjZMa_g8rok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Downloading NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize stemming and lemmatization\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Initialize stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "clean_txt1=[]\n",
        "# Preprocessing steps\n",
        "# Lowercasing\n",
        "txt1 = txt1.lower()\n",
        "# Tokenization\n",
        "tokens = word_tokenize(txt1)\n",
        "# Removing Punctuation, Stopwords, and Numbers\n",
        "tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "# Stemming and Lemmatization\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "# Removing Special Characters\n",
        "cleaned_tokens = [re.sub(r'[^a-zA-Z0-9\\s]', '', token) for token in tokens]\n",
        "# Removing Extra Whitespace\n",
        "cleaned_tokens = [token.strip() for token in cleaned_tokens if token.strip()]\n",
        "# Append preprocessed txt1 to the result\n",
        "clean_txt1.append(cleaned_tokens)\n",
        "# Now, clean_txt contains the preprocessed txt1 data\n",
        "print(clean_txt1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAmJCN_owOXw",
        "outputId": "b5c74ee1-150d-4012-bb3c-74ec77b1a0ff"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['serene', 'lake', 'glistened', 'golden', 'rays', 'setting', 'sun', 'casting', 'mesmerizing', 'reflection', 'surrounding', 'mountains', 'trees', 'swayed', 'gently', 'breeze', 'leaves', 'rustling', 'softly', 'whispering', 'secrets', 'evening', 'air', 'lone', 'boat', 'drifted', 'lazily', 'across', 'water', 'oars', 'dipping', 'rhythmically', 'creating', 'ripples', 'danced', 'fading', 'light']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text1 Vectorization"
      ],
      "metadata": {
        "id": "_oyRc-Pw8vuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Initialize and train Word2Vec model\n",
        "word2vec_model1 = Word2Vec(sentences=clean_txt1, vector_size=100, window=5, min_count=1, sg=0)\n",
        "# Get the word vector for a specific word (e.g., 'rays')\n",
        "word_vector1 = word2vec_model1.wv['rays']\n",
        "# Print the word vector\n",
        "print(\"Word vector for 'rays':\", word_vector1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87Dt02FixlD-",
        "outputId": "f8345ece-e9d9-4869-aeb2-7a0775499bf2"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word vector for 'rays': [-0.00950012  0.00956222 -0.00777076 -0.00264551 -0.00490641 -0.0049667\n",
            " -0.00802359 -0.00778358 -0.00455321 -0.00127536 -0.00510299  0.00614054\n",
            " -0.00951662 -0.0053071   0.00943715  0.00699133  0.00767582  0.00423474\n",
            "  0.00050709 -0.00598114  0.00601878  0.00263503  0.00769943  0.00639384\n",
            "  0.00794257  0.00865741 -0.00989575 -0.0067557   0.00133757  0.0064403\n",
            "  0.00737382  0.00551698  0.00766163 -0.00512557  0.00658441 -0.00410837\n",
            " -0.00905534  0.00914168  0.0013314  -0.00275968 -0.00247784 -0.00422048\n",
            "  0.00481234  0.00440022 -0.00265336 -0.00734188 -0.00356585 -0.00033661\n",
            "  0.00609589 -0.00283734 -0.00012089  0.00087973 -0.00709565  0.002065\n",
            " -0.00143242  0.00280215  0.00484222 -0.00135202 -0.00278014  0.00773865\n",
            "  0.0050456   0.00671352  0.00451564  0.00866716  0.00747497 -0.00108189\n",
            "  0.00874764  0.00460172  0.00544063 -0.00138608 -0.00204132 -0.00442435\n",
            " -0.0085152   0.00303773  0.00888319  0.00891974 -0.00194235  0.00608616\n",
            "  0.00377972 -0.00429597  0.00204292 -0.00543789  0.00820889  0.00543291\n",
            "  0.00318443  0.00410257  0.00865715  0.00727203 -0.00083347 -0.00707277\n",
            "  0.00838047  0.00723358  0.00173047 -0.00134749 -0.00589009 -0.00453309\n",
            "  0.00864797 -0.00313511 -0.00633882  0.00987008]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text2 Preprocessing"
      ],
      "metadata": {
        "id": "0xNwcm8X8ybs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "# why are you running sentences=clean_txt1 for both the paragraphs. i get the value correct.but what is the logic behind that?\n",
        "# Downloading NLTK resources\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "# Initialize stemming and lemmatization\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Initialize stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "clean_txt2=[]\n",
        "# Preprocessing steps\n",
        "# Lowercasing\n",
        "txt2 = txt2.lower()\n",
        "# Tokenization\n",
        "tokens = word_tokenize(txt2)\n",
        "# Removing Punctuation, Stopwords, and Numbers\n",
        "tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "# Stemming and Lemmatization\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "# Removing Special Characters\n",
        "cleaned_tokens = [re.sub(r'[^a-zA-Z0-9\\s]', '', token) for token in tokens]\n",
        "# Removing Extra Whitespace\n",
        "cleaned_tokens = [token.strip() for token in cleaned_tokens if token.strip()]\n",
        "# Append preprocessed txt2 to the result\n",
        "clean_txt2.append(cleaned_tokens)\n",
        "# Now, clean_txt contains the preprocessed txt2 data\n",
        "print(clean_txt2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUIyNKbVxTF8",
        "outputId": "9c51547f-5ef4-456b-bc08-dcb1edfe08fb"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['sun', 'dipped', 'horizon', 'tranquil', 'lake', 'shimmered', 'radiant', 'glow', 'mirroring', 'majestic', 'peaks', 'encircled', 'breezes', 'swept', 'forest', 'causing', 'branches', 'sway', 'sway', 'whispers', 'blending', 'soothing', 'sounds', 'nature', 'meanwhile', 'solitary', 'boat', 'glided', 'gracefully', 'across', 'water', 'paddles', 'slicing', 'surface', 'leaving', 'behind', 'trail', 'gentle', 'waves', 'sparkled', 'twilight']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text2 Vectorization"
      ],
      "metadata": {
        "id": "gLj6dsBn80wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Initialize and train Word2Vec model\n",
        "word2vec_model2 = Word2Vec(sentences=clean_txt2, vector_size=100, window=5, min_count=1, sg=0)\n",
        "# Get the word vector for a specific word (e.g., 'lake')\n",
        "word_vector2 = word2vec_model2.wv['lake']\n",
        "# Print the word vector\n",
        "print(\"Word vector for 'lake':\", word_vector2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdnEJowMxczo",
        "outputId": "8525754e-9131-4ada-d8bd-9c8770b64db3"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word vector for 'lake': [-7.1915817e-03  4.2295568e-03  2.1648116e-03  7.4498607e-03\n",
            " -4.8936559e-03 -4.5641209e-03 -6.0906620e-03  3.2992726e-03\n",
            " -4.5082755e-03  8.5229399e-03 -4.2878119e-03 -9.1028297e-03\n",
            " -4.8086331e-03  6.4238859e-03 -6.3737277e-03 -5.2679847e-03\n",
            " -7.2962642e-03  6.0127759e-03  3.3562265e-03  2.8388314e-03\n",
            " -3.1337836e-03  6.0276142e-03 -6.1495788e-03 -1.9838861e-03\n",
            " -5.9819273e-03 -9.9541235e-04 -2.0221686e-03  8.4835980e-03\n",
            "  7.8231315e-05 -8.5720764e-03 -5.4231454e-03 -6.8743774e-03\n",
            "  2.6964461e-03  9.4519500e-03 -5.8160513e-03  8.2701240e-03\n",
            "  8.5279532e-03 -7.0660952e-03 -8.8832462e-03  9.4653005e-03\n",
            "  8.3708419e-03 -4.6976148e-03 -6.7173666e-03  7.8460453e-03\n",
            "  3.7681770e-03  8.0943014e-03 -7.5760060e-03 -9.5261475e-03\n",
            "  1.5781434e-03 -9.8097688e-03 -4.8853713e-03 -3.4506572e-03\n",
            "  9.6159931e-03  8.6252512e-03 -2.8403746e-03  5.8276341e-03\n",
            "  8.2401754e-03 -2.2644624e-03  9.5253121e-03  7.1571032e-03\n",
            "  2.0429143e-03 -3.8572450e-03 -5.0821430e-03 -3.0472644e-03\n",
            "  7.8850007e-03 -6.1881356e-03 -2.9187417e-03  9.1911620e-03\n",
            "  3.4512910e-03  6.0758851e-03 -8.0335429e-03 -7.4785459e-04\n",
            "  5.5186036e-03 -4.7140149e-03  7.4860090e-03  9.3128709e-03\n",
            " -4.0426292e-04 -2.0538310e-03 -5.8764324e-04 -5.7805176e-03\n",
            " -8.3867451e-03 -1.5130898e-03 -2.5579317e-03  4.3830504e-03\n",
            " -6.8675629e-03  5.4139923e-03 -6.7439363e-03 -7.8230277e-03\n",
            "  8.4767835e-03  8.9244805e-03 -3.4770332e-03  3.4880086e-03\n",
            " -5.7965629e-03 -8.7506967e-03 -5.5180746e-03  6.7471261e-03\n",
            "  6.4126183e-03  9.4344094e-03  7.0561972e-03  6.7551718e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combined vectorization, finding avg vector of the two paragraphs and then finding similarity (cosine) between them"
      ],
      "metadata": {
        "id": "znxnFFwr843_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Combine sentences from both clean_txt1 and clean_txt2\n",
        "combined_sentences = clean_txt1 + clean_txt2\n",
        "\n",
        "# Initialize and train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=combined_sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "# Function to calculate average vector representation for a paragraph\n",
        "def get_average_vector(paragraph, model):\n",
        "    vectors = []\n",
        "    for word in paragraph:\n",
        "        if word in model.wv:\n",
        "            vectors.append(model.wv[word])\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Get average vector representations for both paragraphs\n",
        "avg_vector1 = get_average_vector(clean_txt1[0], word2vec_model)\n",
        "avg_vector2 = get_average_vector(clean_txt2[0], word2vec_model)\n",
        "\n",
        "# Calculate cosine similarity between the average vectors\n",
        "if avg_vector1 is not None and avg_vector2 is not None:\n",
        "    similarity_score = cosine_similarity(avg_vector1.reshape(1, -1), avg_vector2.reshape(1, -1))\n",
        "    similarity_percentage = ((similarity_score[0][0] + 1) / 2) * 100\n",
        "    print(\"Similarity between the two paragraphs:\", similarity_percentage, \"%\")\n",
        "else:\n",
        "    print(\"No word vectors found for one or both paragraphs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ69lk1y2bhH",
        "outputId": "e5b4d5db-58f4-44be-c899-faa83e693b1a"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between the two paragraphs: 58.50183218717575 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why cosine similarity?"
      ],
      "metadata": {
        "id": "iSm6-CB69NCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic Similarity: Cosine Similarity is effective for measuring the similarity between two vectors in a high-dimensional space, such as the vector representations of the paragraphs obtained from word embeddings like Word2Vec or GloVe. It captures the semantic similarity between the paragraphs based on the direction of the vectors, disregarding their magnitudes.\n",
        "\n",
        "Robustness to Length Differences: Cosine Similarity is robust to differences in the length of the paragraphs. Since it calculates the cosine of the angle between the vectors, it is unaffected by the absolute length of the paragraphs.\n",
        "\n",
        "Contextual Similarity: Cosine Similarity considers the overall context and distribution of words in the paragraphs. It evaluates the similarity based on the distributional semantics of the words, which is suitable for capturing the thematic similarities between the paragraphs.\n",
        "\n",
        "Given the descriptive nature of the paragraphs and their thematic similarity, Cosine Similarity would be a suitable choice for comparing their semantic similarity."
      ],
      "metadata": {
        "id": "DZTP89ru8j9x"
      }
    }
  ]
}